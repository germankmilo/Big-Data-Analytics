{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# TALLER 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Cargar los archivos en formato de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Especificar cantidad de documentos\n",
    "cant=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 documentos generados\n",
      "TODOS LOS DOCUMENTOS HAN SIDO CREADOS CON EXITO\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "formato=\".txt\"\n",
    "inicio=0\n",
    "\n",
    "#Creamos la carpeta de las noticias si no existe\n",
    "if not os.path.exists('noticias'): \n",
    "    os.makedirs('noticias')\n",
    "\n",
    "for cont in range(1):\n",
    "    archivo = open(\"reuters21578/reut2-00%d%s\" %(cont,\".sgm\"), \"r\")\n",
    "    \n",
    "    \n",
    "    contenido = archivo.read()\n",
    "    html = BeautifulSoup(contenido, \"lxml\")\n",
    "    entradas = html.find_all('reuters')\n",
    "    #Creamos un documento por cada noticia\n",
    "    \n",
    "    for i,entrada in enumerate(entradas):\n",
    "        inicio=inicio+1\n",
    "        texto = entrada.find('text').getText()\n",
    "        documento= open(\"noticias/noticia%d%s\" %(inicio, formato), \"w\")\n",
    "        documento= open(\"noticias/noticia%d%s\" %(inicio, formato), \"a\")\n",
    "        documento.write(texto)\n",
    "            \n",
    "    documento.close()\n",
    "    print (\"%d documentos generados\" %(inicio))\n",
    "    archivo.close()\n",
    "\n",
    "print (\"TODOS LOS DOCUMENTOS HAN SIDO CREADOS CON EXITO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Eliminar los stopwords (preposiciones, conectores, artículos, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 archivos temporales generados\n",
      "TODOS LOS DOCUMENTOS <<TEMPORALES>> HAN SIDO CREADOS CON EXITO\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "formato=\".txt\"\n",
    "stopwords= \"(\\n|\\t|\\-|\\+|\\/|'s | a | about | above | after | again | against | all | am | an | and | any | are | aren't | as | at | be | because | been | before | being | below | between | both | but | by | can't | cannot | could | couldn't | did | didn't | do | does | doesn't | doing | don't | down | during | each | few | for | from | further | had | hadn't | has | hasn't | have | haven't | having | he | he'd | he'll | he's | her | here | here's | hers | herself | him | himself | his | how | how's | i | i'd | i'll | i'm | i've | if | in | into | is | isn't | it | it's | its | itself | let's | me | more | most | mustn't | my | myself | no | nor | not | of | off | on | once | only | or | other | ought | our | ours | ourselves | out | over | own | same | shan't | she | she'd | she'll | she's | should | shouldn't | so | some | such | than | that | that's | the | their | theirs | them | themselves | then | there | there's | these | they | they'd | they'll | they're | they've | this | those | through | to | tobe | too | under | until | up | very | was | wasn't | we | we'd | we'll | we'be | we're | we've | were | weren't | what | what's | when | when's | where | where's | which | while | who | who's | whom | why | why's | with | won't | would | wouldn't | you | you'd | you'll | you're | you've | your | yours | yourself | yourselves )\"\n",
    "simbolos = \"(\\\\|\\'|\\;|\\:|\\?|\\¿|\\<|\\>|\\,|\\*|\\(|\\)|\\¡|\\!|\\$|\\#|\\||\\%|\\&|\\=|\\[|\\])\" \n",
    "\n",
    "#Creamos la carpeta de los archivos temporales si no existe\n",
    "if not os.path.exists('noticiasTEMP'): \n",
    "    os.makedirs('noticiasTEMP')\n",
    "\n",
    "#Leemos cada archivo, guardamos su informacion en la variable texto y procedemos a limpiarla\n",
    "for cont in range(cant):\n",
    "    archivo = open(\"noticias/noticia%d%s\" %(cont+1,formato), \"r\")\n",
    "    texto=archivo.read()\n",
    "    texto=texto.lower()\n",
    "    texto=' '+texto+' '\n",
    "    texto= texto.replace('.','')\n",
    "    texto= texto.replace('\\\"',' ')\n",
    "    texto=re.sub(simbolos,' ',texto)\n",
    "    texto=re.sub('[0-9]','',texto)\n",
    "    texto=re.sub(' +',' ',texto)\n",
    "    texto=re.sub(\"( '|' )\",' ',texto)\n",
    "    for a in range(5):\n",
    "        texto=re.sub(stopwords,' ',texto)\n",
    "    texto=re.sub(\"'\",' ',texto)    \n",
    "    documento= open(\"noticiasTEMP/noticiaTEMP%d%s\" %(cont+1, formato), \"w\")\n",
    "    documento= open(\"noticiasTEMP/noticiaTEMP%d%s\" %(cont+1, formato), \"a\")\n",
    "    documento.write(texto)\n",
    "    documento.close()\n",
    "    archivo.close()\n",
    "    if cont%1000==0:\n",
    "        print (\"%d archivos temporales generados\"%(cont))\n",
    "\n",
    "\n",
    "print (\"TODOS LOS DOCUMENTOS <<TEMPORALES>> HAN SIDO CREADOS CON EXITO\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with n_samples=2000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tdfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hecho\n"
     ]
    }
   ],
   "source": [
    "lista = []\n",
    "docs = []\n",
    "for cont in range(cant):\n",
    "    file = open(\"noticiasTEMP/noticiaTEMP%d%s\" %(cont+1,\".txt\"), \"r\")\n",
    "    data=file.readlines()\n",
    "    file.close() \n",
    "    data = ''.join(str(e) for e in data)  \n",
    "    docs.append(data)\n",
    "    \n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                             stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(docs[:999])\n",
    "\n",
    "print(\"hecho\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 779)\t0.0114786110493\n",
      "  (0, 158)\t0.0405318467333\n",
      "  (0, 936)\t0.0405318467333\n",
      "  (0, 103)\t0.0559827069769\n",
      "  (0, 719)\t0.0567255198192\n",
      "  (0, 308)\t0.0339593683154\n",
      "  (0, 332)\t0.0488849284847\n",
      "  (0, 296)\t0.0454225933083\n",
      "  (0, 214)\t0.0462619674778\n",
      "  (0, 825)\t0.0465569589669\n",
      "  (0, 188)\t0.0513806242294\n",
      "  (0, 55)\t0.0603481534776\n",
      "  (0, 54)\t0.106875246917\n",
      "  (0, 212)\t0.0949875366674\n",
      "  (0, 229)\t0.32062574075\n",
      "  (0, 923)\t0.347580047856\n",
      "  (0, 51)\t0.136965906289\n",
      "  (0, 998)\t0.250111241956\n",
      "  (0, 372)\t0.0500606243891\n",
      "  (0, 460)\t0.244424642424\n",
      "  (0, 461)\t0.137085533665\n",
      "  (0, 603)\t0.100121248778\n",
      "  (0, 570)\t0.20640408453\n",
      "  (0, 556)\t0.0546351360183\n",
      "  (0, 929)\t0.110575703936\n",
      "  :\t:\n",
      "  (997, 558)\t0.0804389995734\n",
      "  (997, 779)\t0.0227803089105\n",
      "  (997, 262)\t0.203137184921\n",
      "  (997, 521)\t0.027343766694\n",
      "  (997, 541)\t0.155248634985\n",
      "  (997, 993)\t0.13420402757\n",
      "  (997, 184)\t0.0993496933505\n",
      "  (998, 290)\t0.193585186491\n",
      "  (998, 884)\t0.364267505912\n",
      "  (998, 549)\t0.195527321144\n",
      "  (998, 948)\t0.176818420463\n",
      "  (998, 345)\t0.353636840925\n",
      "  (998, 842)\t0.413957509358\n",
      "  (998, 784)\t0.361473825284\n",
      "  (998, 5)\t0.189963976233\n",
      "  (998, 496)\t0.133087208792\n",
      "  (998, 607)\t0.157492362236\n",
      "  (998, 72)\t0.23011574637\n",
      "  (998, 522)\t0.225506706674\n",
      "  (998, 543)\t0.307675766779\n",
      "  (998, 779)\t0.0424386162929\n",
      "  (998, 296)\t0.167935998542\n",
      "  (998, 521)\t0.0509401179452\n",
      "  (998, 541)\t0.144610357919\n",
      "  (998, 800)\t0.0516841744552\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with n_samples=999 and n_features=1000...\n",
      "hecho\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"\n",
    "      % (999, n_features))\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "print(\"hecho\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "vs net mln cts shr revs qtr dlrs avg shrs oper th mths sales note year jan profit reuter gain\n",
      "\n",
      "Topic #1:\n",
      "blah says fed dlrs week rospatch diagnostic dlr canadian mln canada viacom bid boeing amc split crude qtr stock distribution\n",
      "\n",
      "Topic #2:\n",
      "said dlrs company corp mln new unit reuter march contract american agreement york products sale acquisition sales division financial group\n",
      "\n",
      "Topic #3:\n",
      "cts qtly div record pay dividend april prior vs march quarterly sets feb reuter payout corp payable raises fla split\n",
      "\n",
      "Topic #4:\n",
      "oil tonnes said prices opec coffee market talks production government export price report ec output quota quotas delegates wheat industry\n",
      "\n",
      "Topic #5:\n",
      "loss profit oper cts vs revs shr year qtr dlrs th note net includes discontinued nil gain excludes share ended\n",
      "\n",
      "Topic #6:\n",
      "franklin mateo san cts free fund payout tax calif insured income march prior div pay record sets vs note reuter\n",
      "\n",
      "Topic #7:\n",
      "shares stock common said share split company offer offering outstanding exchange stake board shareholders holders securities dlrs mln tender group\n",
      "\n",
      "Topic #8:\n",
      "pct mln bond year january issue eurobond february coupon stg lead december rose manager date yen issues rate priced london\n",
      "\n",
      "Topic #9:\n",
      "billion bank banks debt dlrs brazil said foreign funaro reserves week government year committee commercial surplus payments fed treasury reserve\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
